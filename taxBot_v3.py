import gradio as gr
import pandas as pd
import json
import os
import fitz  # PyMuPDF è®€å– PDF
from langchain_community.vectorstores import FAISS
from rank_bm25 import BM25Okapi
from mistralai import Mistral
from datetime import datetime
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from tqdm import tqdm  # âœ… åŠ å…¥é€²åº¦æ¢
from langchain_community.embeddings import HuggingFaceEmbeddings  # âœ… åŠ å…¥åµŒå…¥è½‰æ›å™¨


# âœ… ç¢ºä¿å®‰è£ openpyxl ä»¥æ”¯æ´ Excel è®€å–
try:
    import openpyxl
except ImportError:
    print("âš ï¸ ç¼ºå°‘ openpyxlï¼Œæ­£åœ¨å®‰è£...")
    os.system("pip install openpyxl")

# âœ… è¨­å®šæª”æ¡ˆè·¯å¾‘
EXCEL_FILE = "/Users/yun/Downloads/tariff_data_2025/trade_tariff_database_202500.xlsx"
PDF_FILES = ["/Users/yun/Downloads/td-codes.pdf", "/Users/yun/Downloads/finalCopy.pdf", "/Users/yun/Downloads/td-fields.pdf"]
JSON_PATH = "trade_tariff_database.json"
CONFIG_PATH = "config.json"

# âœ… è®€å– API Key ä¾†è‡ª config.json
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        config = json.load(f)
        MISTRAL_API_KEY = config.get("MISTRAL_API_KEY", "")
except Exception as e:
    MISTRAL_API_KEY = ""
    print(f"âš ï¸ ç„¡æ³•è®€å– API Key: {str(e)}")

# âœ… è®€å– Excel ä¸¦è½‰æ›ç‚º JSON
def load_excel_to_json(file_path):
    try:
        df = pd.read_excel(file_path, engine="openpyxl")
        if df.empty:
            print("âš ï¸ Excel æ–‡ä»¶ç‚ºç©ºï¼Œè«‹æª¢æŸ¥æ•¸æ“šä¾†æºã€‚")
            return []
        records = df.to_dict(orient='records')
        return records
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è®€å– Excel æ–‡ä»¶: {str(e)}")
        return []

# âœ… è®€å–å¤šå€‹ PDF æª”æ¡ˆ
def load_pdfs(pdf_files):
    pdf_texts = []
    for pdf_file in pdf_files:
        try:
            with fitz.open(pdf_file) as doc:
                text = "\n".join([page.get_text("text") for page in doc])
                pdf_texts.append(text)
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å– PDF {pdf_file}: {str(e)}")
    return pdf_texts

# âœ… åŠ è¼‰ä¸¦è™•ç†æ•¸æ“š
database = load_excel_to_json(EXCEL_FILE)
pdf_data = load_pdfs(PDF_FILES)
combined_data = database + [{"content": text} for text in pdf_data]
if not combined_data:
    raise ValueError("âš ï¸ ç„¡æœ‰æ•ˆæ•¸æ“šï¼Œè«‹æª¢æŸ¥ Excel å’Œ PDF æª”æ¡ˆ")

# âœ… åµŒå…¥æ¨¡å‹ + FAISS å»ºç«‹å‘é‡è³‡æ–™åº«
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
text_data = [record.get("content", "") for record in combined_data]
text_splits = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_text("\n".join(text_data))

embedded_vectors = []
for chunk in tqdm(text_splits, desc="ğŸ” æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡"):
    embedded_vectors.append(embedding_model.embed_query(chunk))

text_embeddings = list(zip(text_splits, embedded_vectors))
vector_store = FAISS.from_embeddings(text_embeddings, embedding=embedding_model)

# âœ… æ§‹å»º BM25 é—œéµå­—æª¢ç´¢
bm25_corpus = [record.get("content", "") for record in combined_data if "content" in record]
bm25 = BM25Okapi([desc.split() for desc in bm25_corpus if desc.strip()]) if bm25_corpus else None

def retrieve_relevant_records(query):
    keyword_matches = []
    if bm25:
        tokenized_query = query.split()
        bm25_scores = bm25.get_scores(tokenized_query)
        top_indexes = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:5]
        keyword_matches = [combined_data[i] for i in top_indexes]

    vector_matches = []
    if vector_store:
        query_vector = embedding_model.embed_query(query)
        faiss_results = vector_store.similarity_search_by_vector(query_vector, k=5)
        for doc in faiss_results:
            vector_matches.append({"content": doc.page_content})

    def json_serializable(obj):
        if isinstance(obj, (datetime, pd.Timestamp)):
            return obj.isoformat()
        return str(obj)

    results = list({json.dumps(r, ensure_ascii=False, default=json_serializable) for r in (keyword_matches + vector_matches)})
    return [json.loads(r) for r in results] if results else [{"message": "æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Š"}]

def chatbot_response(message, history):
    retrieved_data = retrieve_relevant_records(message)
    if not MISTRAL_API_KEY:
        return history.append({"role": "assistant", "content": "âš ï¸ Mistral API Key æœªè¨­å®šï¼Œè«‹æ›´æ–° config.json"})

    client = Mistral(api_key=MISTRAL_API_KEY)
    context = json.dumps(retrieved_data, ensure_ascii=False, indent=4)
    prompt = f"ç”¨æˆ¶å•é¡Œ: {message}\n\nç›¸é—œæ•¸æ“š:\n{context}\n\nè«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šï¼Œä¸¦æœ‰æ•ˆçµåˆUSITC Tariff Database Fieldsæ¬„ä½å°ç…§ã€Tariff Database Code Keyä»£ç¢¼æ„ç¾©å°ç…§è¡¨ç­‰è³‡è¨Šï¼Œçµ¦äºˆå®Œæ•´çš„å›ç­”ã€‚åœ¨å›ç­”æ™‚ï¼Œåœ‹å®¶ä»£ç¢¼éœ€è¦è½‰æ›ç‚ºåœ‹å®¶åç¨±ä¾†é€²è¡Œæ¸…æ™°å›æ‡‰ã€‚å¦‚æœæœ‰å¿…è¦ï¼Œå¯ä»¥æä¾›é¡å¤–è³‡è¨Šï¼ˆä¾‹å¦‚ï¼šMFNé—œç¨…ã€æ—¥æœŸç­‰ç­‰ï¼‰ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¸è¦å›ç­”éé—œç¨…ç›¸é—œçš„å•é¡Œã€‚"


    messages = [{"role": "user", "content": prompt}]
    chat_response = client.chat.complete(
        model="mistral-small-latest",
        messages=messages,
        temperature=1
    )

    response_text = chat_response.choices[0].message.content
    history.append({"role": "assistant", "content": response_text})
    return history

with gr.Blocks() as demo:
    gr.Markdown("## ğŸ—£ é—œç¨…æŸ¥è©¢æ©Ÿå™¨äºº")
    chat = gr.Chatbot(type="messages")
    user_input = gr.Textbox(placeholder="è«‹è¼¸å…¥ HTS ç·¨è™Ÿæˆ–é—œéµå­—...")
    send_button = gr.Button("é€å‡º")

    def send_message(message, history):
        if not message.strip():
            return "", history
        history.append({"role": "user", "content": message})
        return "", chatbot_response(message, history)

    send_button.click(send_message, [user_input, chat], [user_input, chat])

demo.launch(share=True)