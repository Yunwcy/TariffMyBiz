import pandas as pd
import json
import os
import fitz  # PyMuPDF è®€å– PDF
from langchain_community.vectorstores import FAISS
from langchain_huggingface.embeddings import HuggingFaceEmbeddings  # ä½¿ç”¨æœ¬åœ° HuggingFace åµŒå…¥
from rank_bm25 import BM25Okapi
from mistralai import Mistral
from langchain.text_splitter import RecursiveCharacterTextSplitter
from flask import Flask, request, abort, jsonify
from google.cloud import storage
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
import time
import traceback

app = Flask(__name__)

# âœ… ç’°å¢ƒè®Šæ•¸
GCS_BUCKET_NAME = "usa_tax_2"
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")

line_bot_api = None
handler = None

# âœ… å…¨åŸŸè³‡æ–™çµæ§‹
embedding_model = None
vector_store = None
bm25 = None
combined_data = []

# âœ… æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
def check_env():
    required_vars = {
        "GCS_BUCKET_NAME": GCS_BUCKET_NAME,
        "MISTRAL_API_KEY": MISTRAL_API_KEY,
        "LINE_CHANNEL_ACCESS_TOKEN": LINE_CHANNEL_ACCESS_TOKEN,
        "LINE_CHANNEL_SECRET": LINE_CHANNEL_SECRET
    }
    
    missing_vars = [name for name, value in required_vars.items() if not value]
    
    if missing_vars:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸: {', '.join(missing_vars)}")
        print("âŒ è«‹æª¢æŸ¥ Cloud Function è¨­å®š")
        os._exit(1)
    
    print("âœ… ç’°å¢ƒè®Šæ•¸æª¢æŸ¥é€šé")

# âœ… åˆå§‹åŒ– LINE BOT
def setup_linebot():
    global line_bot_api, handler
    line_bot_api = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN)
    handler = WebhookHandler(LINE_CHANNEL_SECRET)
    print("âœ… LINE BOT å·²åˆå§‹åŒ–")

# âœ… å»¶é²è¨»å†Š LINE handler
def register_handler():
    @handler.add(MessageEvent, message=TextMessage)
    def handle_message(event):
        user_msg = event.message.text
        reply = generate_reply(user_msg)
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text=reply))

# âœ… å¾ GCS è®€å– JSON æª”æ¡ˆ
def load_json_from_gcs(bucket_name, file_path):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_path)
    content = blob.download_as_text()
    return json.loads(content)

# âœ… GCS è®€å– PDF
def load_pdfs_from_gcs(bucket_name, pdf_files):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    pdf_texts = []
    for pdf_file in pdf_files:
        blob = bucket.blob(pdf_file)
        content = blob.download_as_bytes()
        with fitz.open(stream=content, filetype="pdf") as doc:
            text = "\n".join([page.get_text("text") for page in doc])
            pdf_texts.append(text)
    return pdf_texts

# âœ… å°‡ FAISS å‘é‡åº«ä¿å­˜åˆ° GCS
def save_vector_store_to_gcs(vector_store, bucket_name, directory_path="vector_store"):
    """å°‡ FAISS å‘é‡åº«ä¿å­˜åˆ° GCS"""
    import tempfile
    import shutil
    import os
    
    # é¦–å…ˆä¿å­˜åˆ°è‡¨æ™‚ç›®éŒ„
    with tempfile.TemporaryDirectory() as temp_dir:
        vector_store.save_local(temp_dir)
        
        # ä¸Šå‚³åˆ° GCS
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        # ä¸Šå‚³ index.faiss æ–‡ä»¶
        index_blob = bucket.blob(f"{directory_path}/index.faiss")
        index_blob.upload_from_filename(os.path.join(temp_dir, "index.faiss"))
        
        # ä¸Šå‚³ index.pkl æ–‡ä»¶
        pkl_blob = bucket.blob(f"{directory_path}/index.pkl")
        pkl_blob.upload_from_filename(os.path.join(temp_dir, "index.pkl"))
    
    print(f"âœ… æˆåŠŸå°‡å‘é‡åº«ä¿å­˜åˆ° GCS: gs://{bucket_name}/{directory_path}/")
    return True

# âœ… å¾ GCS åŠ è¼‰ FAISS å‘é‡åº« (æ”¹é€²ç‰ˆ)
def load_vector_store_from_gcs(embedding_model, bucket_name, directory_path="vector_store"):
    """å¾ GCS åŠ è¼‰ FAISS å‘é‡åº«ï¼Œå¢å¼·å…¼å®¹æ€§å’ŒéŒ¯èª¤è™•ç†"""
    import tempfile
    import os
    from langchain_community.vectorstores import FAISS
    
    try:
        # å‰µå»ºè‡¨æ™‚ç›®éŒ„
        temp_dir = tempfile.mkdtemp()
        
        # å¾ GCS ä¸‹è¼‰æ–‡ä»¶
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        # ä¸‹è¼‰ index.faiss æ–‡ä»¶
        index_blob = bucket.blob(f"{directory_path}/index.faiss")
        index_file_path = os.path.join(temp_dir, "index.faiss")
        index_blob.download_to_filename(index_file_path)
        
        # ä¸‹è¼‰ index.pkl æ–‡ä»¶
        pkl_blob = bucket.blob(f"{directory_path}/index.pkl")
        pkl_file_path = os.path.join(temp_dir, "index.pkl")
        pkl_blob.download_to_filename(pkl_file_path)
        
        print(f"âœ… å·²å¾ GCS ä¸‹è¼‰å‘é‡åº«æ–‡ä»¶: gs://{bucket_name}/{directory_path}/")
        
        # è¼¸å‡ºæ–‡ä»¶å¤§å°ä¿¡æ¯ï¼Œæœ‰åŠ©æ–¼è¨ºæ–·
        faiss_size = os.path.getsize(index_file_path) / (1024 * 1024)
        pkl_size = os.path.getsize(pkl_file_path) / (1024 * 1024)
        print(f"ğŸ“Š å‘é‡åº«æ–‡ä»¶å¤§å°: index.faiss: {faiss_size:.2f}MB, index.pkl: {pkl_size:.2f}MB")
        
        # æ·»åŠ è¨ºæ–·ä»£ç¢¼ï¼šæª¢æŸ¥ç•¶å‰åµŒå…¥æ¨¡å‹çš„å‘é‡ç¶­åº¦
        current_embedding_dim = len(embedding_model.embed_query("è¨ºæ–·å‘é‡ç¶­åº¦"))
        print(f"ğŸ” ç•¶å‰åµŒå…¥æ¨¡å‹å‘é‡ç¶­åº¦: {current_embedding_dim}")
        
        # å˜—è©¦åŠ è¼‰å‘é‡åº«
        try:
            vector_store = FAISS.load_local(temp_dir, embedding_model, allow_dangerous_deserialization=True)
            
            # è¼¸å‡ºä¸€äº›å‘é‡åº«ä¿¡æ¯
            if hasattr(vector_store, 'index') and hasattr(vector_store.index, 'ntotal'):
                print(f"ğŸ“Š åŠ è¼‰çš„å‘é‡åº«åŒ…å« {vector_store.index.ntotal} å€‹å‘é‡")
                
                # æª¢æŸ¥å‘é‡åº«ä¸­å‘é‡çš„ç¶­åº¦
                if hasattr(vector_store.index, 'd'):
                    print(f"ğŸ” å‘é‡åº«ä¸­å‘é‡çš„ç¶­åº¦: {vector_store.index.d}")
                    
                    # å¦‚æœç¶­åº¦ä¸åŒ¹é…ï¼Œçµ¦å‡ºè­¦å‘Š
                    if vector_store.index.d != current_embedding_dim:
                        print(f"âš ï¸ è­¦å‘Šï¼šå‘é‡åº«ç¶­åº¦ ({vector_store.index.d}) èˆ‡ç•¶å‰æ¨¡å‹ç¶­åº¦ ({current_embedding_dim}) ä¸åŒ¹é…")
                        print("å»ºè­°é‡æ–°å»ºç«‹å‘é‡åº«")
            
            print(f"âœ… æˆåŠŸå¾ GCS åŠ è¼‰å‘é‡åº«")
            return vector_store
        
        except Exception as e:
            print(f"âŒ åŠ è¼‰å‘é‡åº«æ–‡ä»¶å¤±æ•—: {str(e)}")
            print("âš ï¸ å¯èƒ½æ˜¯å‘é‡ç¶­åº¦æˆ–æ ¼å¼èˆ‡ç•¶å‰åµŒå…¥æ¨¡å‹ä¸å…¼å®¹")
            traceback.print_exc()
            return None
    
    except Exception as e:
        print(f"âŒ å¾ GCS ä¸‹è¼‰å‘é‡åº«æ–‡ä»¶æ™‚å‡ºéŒ¯: {str(e)}")
        traceback.print_exc()
        return None
    finally:
        # æ¸…ç†è‡¨æ™‚ç›®éŒ„
        if os.path.exists(temp_dir):
            import shutil
            shutil.rmtree(temp_dir)


# âœ… æª¢æŸ¥ GCS ä¸­æ˜¯å¦å­˜åœ¨å‘é‡åº«
def vector_store_exists_in_gcs(bucket_name, directory_path="vector_store"):
    """æª¢æŸ¥ GCS ä¸­æ˜¯å¦å­˜åœ¨å‘é‡åº«"""
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        # æª¢æŸ¥ index.faiss æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        index_blob = bucket.blob(f"{directory_path}/index.faiss")
        index_exists = index_blob.exists()
        
        # æª¢æŸ¥ index.pkl æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        pkl_blob = bucket.blob(f"{directory_path}/index.pkl")
        pkl_exists = pkl_blob.exists()
        
        return index_exists and pkl_exists
    except Exception as e:
        print(f"âŒ æª¢æŸ¥ GCS å‘é‡åº«æ™‚å‡ºéŒ¯: {str(e)}")
        return False

# âœ… å®‰å…¨è™•ç†åµŒå…¥æ‰¹æ¬¡
def safe_embed_documents(embedding_model, texts, batch_size=8, max_texts=8000):
    """å®‰å…¨åœ°è™•ç†æ–‡æœ¬åµŒå…¥ï¼Œæ”¯æ´æ‰¹æ¬¡è™•ç†ï¼Œä¸¦é™åˆ¶æœ€å¤§æ–‡æœ¬æ•¸é‡"""
    
    # é™åˆ¶è™•ç†çš„æ–‡æœ¬æ•¸é‡
    #if len(texts) > max_texts:
    #    print(f"âš ï¸ æ–‡æœ¬æ•¸é‡éå¤š ({len(texts)})ï¼Œé™åˆ¶ç‚º {max_texts} å€‹")
    #    texts = texts[:max_texts]
    
    # å°‡æ–‡æœ¬åˆ†æˆè¼ƒå°çš„æ‰¹æ¬¡
    batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
    all_embeddings = []
    valid_texts = []
    
    for i, batch in enumerate(batches):
        print(f"â–¶ï¸ è™•ç†æ‰¹æ¬¡ {i+1}/{len(batches)}ï¼Œå…± {len(batch)} å€‹æ–‡æœ¬")
        
        try:
            # å˜—è©¦æ‰¹é‡åµŒå…¥
            embeddings = embedding_model.embed_documents(batch)
            
            # æª¢æŸ¥åµŒå…¥çµæœæ˜¯å¦æœ‰æ•ˆ
            if len(embeddings) == len(batch):
                print(f"âœ… æ‰¹æ¬¡ {i+1} åµŒå…¥æˆåŠŸï¼Œç²å¾— {len(embeddings)} å€‹å‘é‡")
                all_embeddings.extend(embeddings)
                valid_texts.extend(batch)
            else:
                print(f"âš ï¸ æ‰¹æ¬¡ {i+1} åµŒå…¥çµæœæ•¸é‡ä¸ç¬¦ï¼Œé æœŸ {len(batch)}ï¼Œå¯¦éš› {len(embeddings)}")
                # åªä½¿ç”¨æˆåŠŸçš„éƒ¨åˆ†
                all_embeddings.extend(embeddings)
                valid_texts.extend(batch[:len(embeddings)])
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {i+1} åµŒå…¥è™•ç†å¤±æ•—: {str(e)}")
            traceback.print_exc()
        
        # æ‰¹æ¬¡ä¹‹é–“æ·»åŠ çŸ­æš«é–“éš”ï¼Œæ¸›è¼•è¨ˆç®—è² æ“”
        time.sleep(0.1)
    
    print(f"ğŸ“Š ç¸½è¨ˆ: è™•ç† {len(texts)} å€‹æ–‡æœ¬ï¼ŒæˆåŠŸåµŒå…¥ {len(all_embeddings)} å€‹")
    return valid_texts, all_embeddings

# âœ… Cold start / Init ç”¨ï¼šåˆå§‹åŒ–è³‡æ–™èˆ‡æ¨¡å‹
def do_init():
    global combined_data, embedding_model, vector_store, bm25

    JSON_FILE = "trade_tariff_database_2025.json"
    PDF_FILES = ["td-codes.pdf", "finalCopy.pdf", "td-fields.pdf"]
    VECTOR_STORE_DIR = "vector_store"  # GCS ä¸­å­˜å„²å‘é‡åº«çš„ç›®éŒ„

    try:
        print("â–¶ï¸ å¾ GCS è¼‰å…¥ JSON èˆ‡ PDF...")
        database = load_json_from_gcs(GCS_BUCKET_NAME, JSON_FILE)
        pdf_data = load_pdfs_from_gcs(GCS_BUCKET_NAME, PDF_FILES)
        combined_data = database + [{"content": text} for text in pdf_data]
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–è³‡æ–™è¼‰å…¥å¤±æ•—ï¼š{e}")
        traceback.print_exc()
        return False

    if not combined_data:
        print("âŒ æœªè®€å–åˆ°ä»»ä½•æœ‰æ•ˆæ•¸æ“š")
        return False

    print("â–¶ï¸ åˆå§‹åŒ– HuggingFace æœ¬åœ°åµŒå…¥æ¨¡å‹...")
    try:
        # ä½¿ç”¨æœ¬åœ° HuggingFaceEmbeddings æ¨¡å‹
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"  
        )
        
        # æ¸¬è©¦åµŒå…¥æ¨¡å‹
        test_result = embedding_model.embed_query("æ¸¬è©¦åµŒå…¥æ¨¡å‹")
        if isinstance(test_result, list) and len(test_result) > 0:
            print(f"âœ… åµŒå…¥æ¨¡å‹æ¸¬è©¦æˆåŠŸï¼Œå‘é‡ç¶­åº¦: {len(test_result)}")
        else:
            print(f"âš ï¸ åµŒå…¥æ¨¡å‹æ¸¬è©¦çµæœæ ¼å¼ç•°å¸¸: {type(test_result)}")
            embedding_model = None
            
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        traceback.print_exc()
        embedding_model = None
    
    # æª¢æŸ¥æ˜¯å¦å­˜åœ¨é å…ˆå»ºç«‹çš„å‘é‡åº«
    if embedding_model:
        print("â–¶ï¸ æª¢æŸ¥ GCS ä¸­æ˜¯å¦å­˜åœ¨å‘é‡åº«æ–‡ä»¶...")
        if vector_store_exists_in_gcs(GCS_BUCKET_NAME, VECTOR_STORE_DIR):
            print("âœ… ç™¼ç¾å‘é‡åº«æ–‡ä»¶ï¼Œæ­£åœ¨å˜—è©¦åŠ è¼‰...")
            
            # è¨˜éŒ„ç•¶å‰æ™‚é–“ï¼Œç”¨æ–¼è¨ˆç®—åŠ è¼‰æ™‚é–“
            start_time = time.time()
            
            vector_store = load_vector_store_from_gcs(embedding_model, GCS_BUCKET_NAME, VECTOR_STORE_DIR)
            
            if vector_store:
                load_time = time.time() - start_time
                print(f"âœ… æˆåŠŸåŠ è¼‰å‘é‡åº«ï¼Œè€—æ™‚ {load_time:.2f} ç§’")
                
                # å˜—è©¦æª¢æŸ¥å‘é‡ç¶­åº¦æ˜¯å¦åŒ¹é…
                test_embedding = embedding_model.embed_query("æ¸¬è©¦")
                expected_dim = len(test_embedding)
                
                # å˜—è©¦å¾å‘é‡åº«ç²å–ä¸€å€‹å‘é‡ä¾†æª¢æŸ¥ç¶­åº¦
                try:
                    # ä½¿ç”¨ä¸€å€‹ç°¡å–®æŸ¥è©¢ä¾†æª¢æŸ¥å‘é‡
                    test_results = vector_store.similarity_search("test", k=1)
                    if test_results:
                        print(f"âœ… å‘é‡åº«å¯ä»¥æ­£å¸¸æœç´¢ï¼Œè¼‰å…¥å®Œæˆ")
                    else:
                        print("âš ï¸ å‘é‡åº«åŠ è¼‰æˆåŠŸï¼Œä½†æœç´¢æ¸¬è©¦è¿”å›ç©ºçµæœ")
                except Exception as e:
                    print(f"âš ï¸ å‘é‡åº«åŠ è¼‰å¾Œï¼Œæœç´¢æ¸¬è©¦å¤±æ•—: {str(e)}")
                    print("âš ï¸ ç¹¼çºŒä½¿ç”¨å‘é‡åº«ï¼Œä½†å¯èƒ½å­˜åœ¨å…¼å®¹æ€§å•é¡Œ")
            else:
                print("âš ï¸ åŠ è¼‰å‘é‡åº«å¤±æ•—ï¼Œå°‡é‡æ–°å»ºç«‹")
                vector_store = None
        else:
            print("â„¹ï¸ æœªæ‰¾åˆ°ç¾æœ‰å‘é‡åº«ï¼Œå°‡å»ºç«‹æ–°çš„å‘é‡åº«")
            vector_store = None
    
    # å¦‚æœæ²’æœ‰åŠ è¼‰æˆåŠŸï¼Œå‰‡é‡æ–°å»ºç«‹å‘é‡åº«
    if embedding_model and not vector_store:
        # æº–å‚™æ–‡æœ¬æ•¸æ“š
        text_data = [record.get("content", "") for record in combined_data]
        text_splits = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_text("\n".join(text_data))
        print(f"â–¶ï¸ æ–‡æœ¬æ‹†åˆ†å®Œæˆï¼Œå…± {len(text_splits)} å€‹ç‰‡æ®µ")
        
        # é™åˆ¶è™•ç†çš„æ–‡æœ¬æ•¸é‡ï¼Œé¿å…è¶…æ™‚
        #max_texts = 8000  # å¯ä»¥æ ¹æ“šæ‚¨çš„ Cloud Function è³‡æºä¾†èª¿æ•´
        #if len(text_splits) > max_texts:
        #    print(f"âš ï¸ é™åˆ¶è™•ç†çš„æ–‡æœ¬æ•¸é‡ç‚º {max_texts}ï¼ˆåŸæœ‰ {len(text_splits)} å€‹ï¼‰")
        #    text_splits = text_splits[:max_texts]
        
        try:
            # ä½¿ç”¨å®‰å…¨çš„æ–¹å¼è™•ç†åµŒå…¥
            valid_texts, embeddings = safe_embed_documents(embedding_model, text_splits, batch_size=8)
            
            if valid_texts and embeddings:
                # å»ºç«‹å‘é‡åº«
                text_embeddings = list(zip(valid_texts, embeddings))
                try:
                    vector_store = FAISS.from_embeddings(text_embeddings, embedding_model)
                    print(f"âœ… æˆåŠŸå»ºç«‹å‘é‡åº«ï¼ŒåŒ…å« {len(text_embeddings)} å€‹åµŒå…¥")
                    
                    # å°‡å‘é‡åº«ä¿å­˜åˆ° GCS
                    print("â–¶ï¸ å°‡å‘é‡åº«ä¿å­˜åˆ° GCS...")
                    if save_vector_store_to_gcs(vector_store, GCS_BUCKET_NAME, VECTOR_STORE_DIR):
                        print("âœ… å‘é‡åº«å·²æˆåŠŸä¿å­˜åˆ° GCSï¼Œä¸‹æ¬¡å°‡ç›´æ¥åŠ è¼‰")
                    else:
                        print("âš ï¸ å‘é‡åº«ä¿å­˜å¤±æ•—ï¼Œä½†æœ¬æ¬¡ä»å¯ä½¿ç”¨")
                        
                except Exception as e:
                    print(f"âŒ å»ºç«‹å‘é‡åº«å¤±æ•—: {str(e)}")
                    traceback.print_exc()
                    vector_store = None
            else:
                print("âŒ æ²’æœ‰æœ‰æ•ˆçš„åµŒå…¥çµæœï¼Œç„¡æ³•å»ºç«‹å‘é‡åº«")
                vector_store = None
        except Exception as e:
            print(f"âŒ åµŒå…¥è™•ç†å¤±æ•—: {str(e)}")
            traceback.print_exc()
            vector_store = None
    else:
        if not embedding_model:
            print("âš ï¸ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³éå‘é‡åº«å»ºç«‹/åŠ è¼‰")
    
    # å»ºç«‹ BM25 ç´¢å¼•ï¼ˆä½œç‚ºå‚™ç”¨æœç´¢æ–¹æ³•ï¼‰
    print("â–¶ï¸ å»ºç«‹ BM25 é—œéµå­—æœå°‹ç´¢å¼•...")
    try:
        bm25_corpus = [record.get("content", "") for record in combined_data if "content" in record]
        if bm25_corpus:
            tokenized_corpus = [doc.split() for doc in bm25_corpus if doc.strip()]
            if tokenized_corpus:
                bm25 = BM25Okapi(tokenized_corpus)
                print(f"âœ… æˆåŠŸå»ºç«‹ BM25 ç´¢å¼•ï¼ŒåŒ…å« {len(tokenized_corpus)} å€‹æ–‡æª”")
            else:
                print("âŒ æ²’æœ‰æœ‰æ•ˆçš„åˆ†è©æ–‡æœ¬ï¼Œç„¡æ³•å»ºç«‹ BM25 ç´¢å¼•")
                bm25 = None
        else:
            print("âŒ æ²’æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å…§å®¹ï¼Œç„¡æ³•å»ºç«‹ BM25 ç´¢å¼•")
            bm25 = None
    except Exception as e:
        print(f"âŒ å»ºç«‹ BM25 ç´¢å¼•å¤±æ•—: {str(e)}")
        traceback.print_exc()
        bm25 = None
    
    # å¦‚æœæ‰€æœ‰ç´¢å¼•éƒ½å¤±æ•—ï¼Œå‰‡è¿”å›å¤±æ•—
    if not vector_store and not bm25:
        print("âŒ å‘é‡åº«å’Œ BM25 ç´¢å¼•éƒ½åˆå§‹åŒ–å¤±æ•—ï¼Œç³»çµ±å°‡ç„¡æ³•æ­£å¸¸å·¥ä½œ")
        return False
    
    print("âœ… åˆå§‹åŒ–å®Œæˆ")
    return True

# âœ… Webhookï¼šLINE è¨Šæ¯å…¥å£
@app.route("/callback", methods=['POST'])
def callback(request):
    signature = request.headers.get('X-Line-Signature', '')
    body = request.get_data(as_text=True)
    print(f"âœ… æ”¶åˆ° LINE Webhook è«‹æ±‚")
    
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        print(f"âŒ LINE ç°½åé©—è­‰å¤±æ•—")
        abort(400)
    except Exception as e:
        print(f"âŒ LINE Webhook è™•ç†å¤±æ•—: {str(e)}")
        traceback.print_exc()
        abort(500)
    
    return 'OK'

# âœ… æ‰‹å‹•åˆå§‹åŒ– API
@app.route("/init", methods=['POST'])
def initialize():
    success = do_init()
    if success:
        return jsonify({"status": "success", "msg": "è³‡æ–™èˆ‡æ¨¡å‹åˆå§‹åŒ–å®Œæˆ"})
    else:
        return jsonify({"status": "fail", "msg": "åˆå§‹åŒ–å¤±æ•—"}), 500

# âœ… å¼·åˆ¶é‡å»ºå‘é‡åº« API
@app.route("/rebuild-vectors", methods=['POST'])
def rebuild_vectors():
    """å¼·åˆ¶é‡å»ºå‘é‡åº«çš„ API ç«¯é»"""
    global vector_store, embedding_model
    
    # æª¢æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–
    if not embedding_model:
        return jsonify({
            "status": "error", 
            "msg": "åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå‘¼å« /init"
        }), 400
    
    try:
        # æº–å‚™æ–‡æœ¬æ•¸æ“š
        text_data = [record.get("content", "") for record in combined_data]
        text_splits = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_text("\n".join(text_data))
        
        # é™åˆ¶è™•ç†çš„æ–‡æœ¬æ•¸é‡ï¼Œé¿å…è¶…æ™‚
        #max_texts = 8000  # å¯ä»¥æ ¹æ“šæ‚¨çš„ Cloud Function è³‡æºä¾†èª¿æ•´
        #if len(text_splits) > max_texts:
        #    text_splits = text_splits[:max_texts]
        
        # ä½¿ç”¨å®‰å…¨çš„æ–¹å¼è™•ç†åµŒå…¥
        valid_texts, embeddings = safe_embed_documents(embedding_model, text_splits, batch_size=8)
        
        if not valid_texts or not embeddings:
            return jsonify({
                "status": "error", 
                "msg": "åµŒå…¥è™•ç†å¤±æ•—ï¼Œæ²’æœ‰ç²å–åˆ°æœ‰æ•ˆçš„åµŒå…¥å‘é‡"
            }), 500
        
        # å»ºç«‹å‘é‡åº«
        text_embeddings = list(zip(valid_texts, embeddings))
        vector_store = FAISS.from_embeddings(text_embeddings, embedding_model)
        
        # å°‡å‘é‡åº«ä¿å­˜åˆ° GCS
        VECTOR_STORE_DIR = "vector_store"
        save_success = save_vector_store_to_gcs(vector_store, GCS_BUCKET_NAME, VECTOR_STORE_DIR)
        
        if not save_success:
            return jsonify({
                "status": "partial_success", 
                "msg": "å‘é‡åº«å·²é‡å»ºä½†ä¿å­˜å¤±æ•—ï¼Œé‡å•Ÿå¾Œå°‡éœ€è¦é‡æ–°å»ºç«‹"
            })
        
        return jsonify({
            "status": "success", 
            "msg": f"æˆåŠŸé‡å»ºä¸¦ä¿å­˜å‘é‡åº«ï¼ŒåŒ…å« {len(text_embeddings)} å€‹åµŒå…¥å‘é‡"
        })
        
    except Exception as e:
        traceback.print_exc()
        return jsonify({
            "status": "error", 
            "msg": f"é‡å»ºå‘é‡åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        }), 500

# âœ… å¥åº·æª¢æŸ¥ API
@app.route("/healthz", methods=['GET'])
def healthz():
    return "OK"

@app.route("/", methods=['GET'])
def index():
    return "Service is running"

# âœ… æª¢ç´¢è³‡æ–™
def retrieve_relevant_records(query):
    keyword_matches, vector_matches = [], []
    
    # ä½¿ç”¨ BM25 é€²è¡Œé—œéµå­—æœå°‹
    if bm25:
        try:
            print(f"â–¶ï¸ åŸ·è¡Œ BM25 é—œéµå­—æœå°‹...")
            tokenized_query = query.split()
            
            # ç¢ºä¿æŸ¥è©¢ä¸ç‚ºç©º
            if not tokenized_query:
                tokenized_query = ["empty"]
            
            bm25_scores = bm25.get_scores(tokenized_query)
            top_indexes = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:5]
            
            # ç²å–å°æ‡‰çš„æ–‡æª”
            for i in top_indexes:
                if i < len(combined_data):
                    keyword_matches.append(combined_data[i])
            
            print(f"âœ… BM25 æœå°‹æ‰¾åˆ° {len(keyword_matches)} å€‹åŒ¹é…é …")
        except Exception as e:
            print(f"âš ï¸ BM25 æœå°‹å¤±æ•—: {str(e)}")
            traceback.print_exc()
    
    # ä½¿ç”¨å‘é‡åº«é€²è¡Œèªæ„æœå°‹
    if vector_store:
        try:
            print(f"â–¶ï¸ åŸ·è¡Œå‘é‡ç›¸ä¼¼åº¦æœå°‹...")
            faiss_results = vector_store.similarity_search(query, k=5)
            vector_matches = [{"content": doc.page_content} for doc in faiss_results]
            print(f"âœ… å‘é‡æœå°‹æ‰¾åˆ° {len(vector_matches)} å€‹åŒ¹é…é …")
        except Exception as e:
            print(f"âš ï¸ å‘é‡æœå°‹å¤±æ•—: {str(e)}")
            traceback.print_exc()
    
    # åˆä½µæœå°‹çµæœä¸¦å»é‡
    results = list({json.dumps(r, ensure_ascii=False) for r in (keyword_matches + vector_matches)})
    
    if not results:
        print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çµæœ")
        return [{"message": "æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Š"}]
    
    return [json.loads(r) for r in results]

# âœ… ä½¿ç”¨ Mistral API ç”Ÿæˆå›æ‡‰
def generate_reply(message):
    # æª¢æŸ¥ç³»çµ±æ˜¯å¦å·²åˆå§‹åŒ–
    if not (embedding_model or bm25):
        return "âš ï¸ ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå‘¼å« /init"

    try:
        # æª¢ç´¢ç›¸é—œè¨˜éŒ„
        retrieved_data = retrieve_relevant_records(message)
        
        # æº–å‚™ Mistral API è«‹æ±‚
        client = Mistral(api_key=MISTRAL_API_KEY)
        
        # å°‡æª¢ç´¢çµæœè½‰æ›ç‚ºä¸Šä¸‹æ–‡
        context = json.dumps(retrieved_data, ensure_ascii=False, indent=2)
        
        # æ§‹å»ºæç¤ºè©
        prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¾åœ‹åœ‹éš›è²¿æ˜“å§”å“¡æœƒï¼ˆUnited States International Trade Commission / USITCï¼‰é—œç¨…æ•¸æ“šå°ˆå®¶åŠ©æ‰‹ã€‚ä½ çš„ä»»å‹™æ˜¯å¾æä¾›çš„æ•¸æ“šä¸­ï¼Œæº–ç¢ºä¸”æ¸…æ™°åœ°å›ç­”èˆ‡é—œç¨…ã€è²¿æ˜“ç›¸é—œçš„å•é¡Œã€‚

                    è™•ç†å•é¡Œæ™‚ï¼Œè«‹éµå¾ªä»¥ä¸‹æŒ‡å°åŸå‰‡ï¼š

                    1. æ•¸æ“šä¾†æºèˆ‡å¯é æ€§
                    - åƒ…ä½¿ç”¨æä¾›çš„æ•¸æ“šåº«å…§å®¹ä½œç‚ºå›ç­”ä¾æ“š
                    - å¦‚æœæ•¸æ“šä¸è¶³ï¼Œæ˜ç¢ºèªªæ˜ç„¡æ³•å®Œå…¨å›ç­”

                    2. å›ç­”æ ¼å¼è¦æ±‚
                    - ä½¿ç”¨æ¸…æ™°çš„åˆ—é»æ ¼å¼
                    - é¿å…ä½¿ç”¨è¤‡é›œçš„ç¬¦è™Ÿ
                    - å°‡å°ˆæ¥­è¡“èªè½‰æ›ç‚ºæ˜“æ–¼ç†è§£çš„èªè¨€
                    - å„ªå…ˆæä¾›æœ€é—œéµå’Œæœ€ç›¸é—œçš„ä¿¡æ¯

                    3. å›ç­”å¿…é ˆåŒ…å«
                    - å…·é«”çš„é—œç¨…æ•¸æ“š
                    - åœ‹å®¶ä»£ç¢¼çš„å°æ‡‰åœ‹å®¶åç¨±
                    - ç›¸é—œçš„ MFNï¼ˆæœ€æƒ åœ‹ï¼‰é—œç¨…ä¿¡æ¯
                    - å¯èƒ½çš„ç”Ÿæ•ˆæ—¥æœŸ

                    4. é¡å¤–è¨Šæ¯è£œå……
                    - å¦‚æœ‰ç›¸é—œçš„é™„åŠ è²¿æ˜“èƒŒæ™¯ä¿¡æ¯ï¼Œå¯ä»¥ç°¡è¦è£œå……
                    - æä¾›æ•¸æ“šçš„ä¾†æºå’Œåƒè€ƒä¾æ“š

                    5. é™åˆ¶æ¢ä»¶
                    - åš´æ ¼é™å®šåœ¨é—œç¨…å’Œåœ‹éš›è²¿æ˜“é ˜åŸŸ
                    - å°æ–¼è¶…å‡ºç¯„åœçš„å•é¡Œï¼Œç¦®è²Œåœ°æ‹’çµ•å›ç­”

                    ç”¨æˆ¶å•é¡Œ: {message}

                    ç›¸é—œæ•¸æ“š:
                    {context}

                    è«‹æ ¹æ“šä¸Šè¿°æŒ‡å°åŸå‰‡ï¼Œæä¾›ä¸€å€‹å°ˆæ¥­ã€ç²¾ç¢ºä¸”æ˜“æ–¼ç†è§£çš„å›ç­”ã€‚"""

        # ç™¼é€è«‹æ±‚åˆ° Mistral API
        messages = [{"role": "user", "content": prompt}]
        
        try:
            chat_response = client.chat.complete(
                model="mistral-small-latest", 
                messages=messages, 
                temperature=0.3,
                max_tokens=300
            )
            return chat_response.choices[0].message.content
        except Exception as e:
            print(f"âŒ Mistral API èª¿ç”¨å¤±æ•—: {str(e)}")
            traceback.print_exc()
            return f"âš ï¸ ç„¡æ³•ç”Ÿæˆå›æ‡‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚æŠ€è¡“éŒ¯èª¤: {str(e)[:100]}..."
    
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        traceback.print_exc()
        return "âš ï¸ è™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

# âœ… ç¨‹å¼é€²å…¥é»ï¼šåˆå§‹åŒ–ç³»çµ±
check_env()
setup_linebot()
print("ğŸ“¦ æ­£åœ¨è‡ªå‹•åˆå§‹åŒ–ç³»çµ±è³‡æ–™...")
do_init()
register_handler()

# âœ… æœ¬åœ°æ¸¬è©¦ç”¨
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port)