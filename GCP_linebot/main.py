import pandas as pd
import json
import os
import fitz  # PyMuPDF è®€å– PDF
from langchain_community.vectorstores import FAISS
from langchain_huggingface.embeddings import HuggingFaceEmbeddings  # ä½¿ç”¨æœ¬åœ° HuggingFace åµŒå…¥
from rank_bm25 import BM25Okapi
from mistralai import Mistral
from langchain.text_splitter import RecursiveCharacterTextSplitter
from flask import Flask, request, abort, jsonify
from google.cloud import storage
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage
import time
import traceback
from deep_translator import GoogleTranslator
import re
from fuzzywuzzy import fuzz

app = Flask(__name__)

# âœ… ç’°å¢ƒè®Šæ•¸
GCS_BUCKET_NAME = "usa_tax_2"
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET")

line_bot_api = None
handler = None

# âœ… å…¨åŸŸè³‡æ–™çµæ§‹
embedding_model = None
vector_store = None
bm25 = None
combined_data = []

def json_serializable(obj):
    try:
        return str(obj)
    except:
        return None

# âœ… ç¿»è­¯æŸ¥è©¢ç‚ºè‹±æ–‡ï¼ˆä½¿ç”¨å¥—ä»¶ï¼‰
def translate_to_english(text):
    try:
        return GoogleTranslator(source='auto', target='en').translate(text)
    except Exception as e:
        print(f"âš ï¸ ç¿»è­¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return text

# âœ… æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
def check_env():
    required_vars = {
        "GCS_BUCKET_NAME": GCS_BUCKET_NAME,
        "MISTRAL_API_KEY": MISTRAL_API_KEY,
        "LINE_CHANNEL_ACCESS_TOKEN": LINE_CHANNEL_ACCESS_TOKEN,
        "LINE_CHANNEL_SECRET": LINE_CHANNEL_SECRET
    }
    
    missing_vars = [name for name, value in required_vars.items() if not value]
    
    if missing_vars:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„ç’°å¢ƒè®Šæ•¸: {', '.join(missing_vars)}")
        print("âŒ è«‹æª¢æŸ¥ Cloud Function è¨­å®š")
        os._exit(1)
    
    print("âœ… ç’°å¢ƒè®Šæ•¸æª¢æŸ¥é€šé")

# âœ… åˆå§‹åŒ– LINE BOT
def setup_linebot():
    global line_bot_api, handler
    line_bot_api = LineBotApi(LINE_CHANNEL_ACCESS_TOKEN)
    handler = WebhookHandler(LINE_CHANNEL_SECRET)
    print("âœ… LINE BOT å·²åˆå§‹åŒ–")

# âœ… å»¶é²è¨»å†Š LINE handler
def register_handler():
    @handler.add(MessageEvent, message=TextMessage)
    def handle_message(event):
        user_msg = event.message.text
        reply = generate_reply(user_msg)
        line_bot_api.reply_message(event.reply_token, TextSendMessage(text=reply))

# âœ… å¾ GCS è®€å– JSON æª”æ¡ˆ
def load_json_from_gcs(bucket_name, file_path):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_path)
    content = blob.download_as_text()
    return json.loads(content)

# âœ… GCS è®€å– PDF
def load_pdfs_from_gcs(bucket_name, pdf_files):
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    pdf_texts = []
    for pdf_file in pdf_files:
        blob = bucket.blob(pdf_file)
        content = blob.download_as_bytes()
        with fitz.open(stream=content, filetype="pdf") as doc:
            text = "\n".join([page.get_text("text") for page in doc])
            pdf_texts.append(text)
    return pdf_texts

# âœ… å°‡ FAISS å‘é‡åº«ä¿å­˜åˆ° GCS
def save_vector_store_to_gcs(vector_store, bucket_name, directory_path="vector_store"):
    """å°‡ FAISS å‘é‡åº«ä¿å­˜åˆ° GCS"""
    import tempfile
    import shutil
    import os
    
    # é¦–å…ˆä¿å­˜åˆ°è‡¨æ™‚ç›®éŒ„
    with tempfile.TemporaryDirectory() as temp_dir:
        vector_store.save_local(temp_dir)
        
        # ä¸Šå‚³åˆ° GCS
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        # ä¸Šå‚³ index.faiss æ–‡ä»¶
        index_blob = bucket.blob(f"{directory_path}/index.faiss")
        index_blob.upload_from_filename(os.path.join(temp_dir, "index.faiss"))
        
        # ä¸Šå‚³ index.pkl æ–‡ä»¶
        pkl_blob = bucket.blob(f"{directory_path}/index.pkl")
        pkl_blob.upload_from_filename(os.path.join(temp_dir, "index.pkl"))
    
    print(f"âœ… æˆåŠŸå°‡å‘é‡åº«ä¿å­˜åˆ° GCS: gs://{bucket_name}/{directory_path}/")
    return True

# âœ… å¾ GCS åŠ è¼‰ FAISS å‘é‡åº« (æ”¹é€²ç‰ˆ)
def load_vector_store_from_gcs(embedding_model, bucket_name, directory_path="vector_store"):
    """å¾ GCS åŠ è¼‰ FAISS å‘é‡åº«ï¼Œå¢å¼·å…¼å®¹æ€§å’ŒéŒ¯èª¤è™•ç†"""
    import tempfile
    import os
    from langchain_community.vectorstores import FAISS
    
    try:
        # å‰µå»ºè‡¨æ™‚ç›®éŒ„
        temp_dir = tempfile.mkdtemp()
        
        # å¾ GCS ä¸‹è¼‰æ–‡ä»¶
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        # ä¸‹è¼‰ index.faiss æ–‡ä»¶
        index_blob = bucket.blob(f"{directory_path}/index.faiss")
        pkl_blob = bucket.blob(f"{directory_path}/index.pkl")

        # âœ… åŠ ä¸Šæ­¤æ®µæª¢æŸ¥
        if not index_blob.exists() or not pkl_blob.exists():
            print(f"âŒ GCS ä¸Šæ‰¾ä¸åˆ° index.faiss æˆ– index.pkl")
            return None

        # æ¥è‘—ä¸‹è¼‰åˆ°æš«å­˜æª”
        index_file_path = os.path.join(temp_dir, "index.faiss")
        index_blob.download_to_filename(index_file_path)

        pkl_file_path = os.path.join(temp_dir, "index.pkl")
        pkl_blob.download_to_filename(pkl_file_path)

        print(f"âœ… å·²å¾ GCS ä¸‹è¼‰å‘é‡åº«æ–‡ä»¶: gs://{bucket_name}/{directory_path}/")
        print(f"ğŸ“ index.faiss å­˜åœ¨æ–¼æœ¬åœ°: {os.path.exists(index_file_path)}")
        print(f"ğŸ“ index.pkl   å­˜åœ¨æ–¼æœ¬åœ°: {os.path.exists(pkl_file_path)}")
        
        # è¼¸å‡ºæ–‡ä»¶å¤§å°ä¿¡æ¯ï¼Œæœ‰åŠ©æ–¼è¨ºæ–·
        faiss_size = os.path.getsize(index_file_path) / (1024 * 1024)
        pkl_size = os.path.getsize(pkl_file_path) / (1024 * 1024)
        print(f"ğŸ“Š å‘é‡åº«æ–‡ä»¶å¤§å°: index.faiss: {faiss_size:.2f}MB, index.pkl: {pkl_size:.2f}MB")
        
        # æ·»åŠ è¨ºæ–·ä»£ç¢¼ï¼šæª¢æŸ¥ç•¶å‰åµŒå…¥æ¨¡å‹çš„å‘é‡ç¶­åº¦
        current_embedding_dim = len(embedding_model.embed_query("è¨ºæ–·å‘é‡ç¶­åº¦"))
        print(f"ğŸ” ç•¶å‰åµŒå…¥æ¨¡å‹å‘é‡ç¶­åº¦: {current_embedding_dim}")
        
        # å˜—è©¦åŠ è¼‰å‘é‡åº«
        try:
            # å˜—è©¦åŠ è¼‰å‘é‡åº«
            try:
                vector_store = FAISS.load_local(temp_dir, embedding_model, allow_dangerous_deserialization=True)
                print("âœ… å‘é‡åº«è¼‰å…¥æˆåŠŸ")
                return vector_store
            except Exception as e:
                print("âŒ å‘é‡åº«è¼‰å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤:")
                traceback.print_exc()
                return None
            
            # è¼¸å‡ºä¸€äº›å‘é‡åº«ä¿¡æ¯
            if hasattr(vector_store, 'index') and hasattr(vector_store.index, 'ntotal'):
                print(f"ğŸ“Š åŠ è¼‰çš„å‘é‡åº«åŒ…å« {vector_store.index.ntotal} å€‹å‘é‡")
                
                # æª¢æŸ¥å‘é‡åº«ä¸­å‘é‡çš„ç¶­åº¦
                if hasattr(vector_store.index, 'd'):
                    print(f"ğŸ” å‘é‡åº«ä¸­å‘é‡çš„ç¶­åº¦: {vector_store.index.d}")
                    
                    # å¦‚æœç¶­åº¦ä¸åŒ¹é…ï¼Œçµ¦å‡ºè­¦å‘Š
                    if vector_store.index.d != current_embedding_dim:
                        print(f"âŒ éŒ¯èª¤ï¼šå‘é‡åº«ç¶­åº¦ ({vector_store.index.d}) èˆ‡åµŒå…¥æ¨¡å‹ç¶­åº¦ ({current_embedding_dim}) ä¸ç¬¦")
                        print("â›” çµ‚æ­¢åŠ è¼‰ï¼Œè¿”å› Noneï¼Œè«‹é‡æ–°å»ºç«‹å‘é‡åº«")
                        return None

            
            print(f"âœ… æˆåŠŸå¾ GCS åŠ è¼‰å‘é‡åº«")
            return vector_store
        
        except Exception as e:
            print(f"âŒ åŠ è¼‰å‘é‡åº«æ–‡ä»¶å¤±æ•—: {str(e)}")
            print("âš ï¸ å¯èƒ½æ˜¯å‘é‡ç¶­åº¦æˆ–æ ¼å¼èˆ‡ç•¶å‰åµŒå…¥æ¨¡å‹ä¸å…¼å®¹")
            traceback.print_exc()
            return None
    
    except Exception as e:
        print(f"âŒ å¾ GCS ä¸‹è¼‰å‘é‡åº«æ–‡ä»¶æ™‚å‡ºéŒ¯: {str(e)}")
        traceback.print_exc()
        return None
    finally:
        # æ¸…ç†è‡¨æ™‚ç›®éŒ„
        if os.path.exists(temp_dir):
            import shutil
            shutil.rmtree(temp_dir)


# âœ… æª¢æŸ¥ GCS ä¸­æ˜¯å¦å­˜åœ¨å‘é‡åº«
def vector_store_exists_in_gcs(bucket_name, directory_path="vector_store"):
    """æª¢æŸ¥ GCS ä¸­æ˜¯å¦å­˜åœ¨å‘é‡åº«"""
    try:
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        # æª¢æŸ¥ index.faiss æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        index_blob = bucket.blob(f"{directory_path}/index.faiss")
        index_exists = index_blob.exists()
        
        # æª¢æŸ¥ index.pkl æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        pkl_blob = bucket.blob(f"{directory_path}/index.pkl")
        pkl_exists = pkl_blob.exists()

        if not index_blob.exists() or not pkl_blob.exists():
            print("GCS ä¸­ç¼ºå°‘å‘é‡åº«æª”æ¡ˆï¼Œéœ€é‡å»º")
        
        return index_exists and pkl_exists
    except Exception as e:
        print(f"âŒ æª¢æŸ¥ GCS å‘é‡åº«æ™‚å‡ºéŒ¯: {str(e)}")
        return False

# âœ… å®‰å…¨è™•ç†åµŒå…¥æ‰¹æ¬¡
def safe_embed_documents(embedding_model, texts, batch_size=8, max_texts=8000):
    """å®‰å…¨åœ°è™•ç†æ–‡æœ¬åµŒå…¥ï¼Œæ”¯æ´æ‰¹æ¬¡è™•ç†ï¼Œä¸¦é™åˆ¶æœ€å¤§æ–‡æœ¬æ•¸é‡"""
    
    # é™åˆ¶è™•ç†çš„æ–‡æœ¬æ•¸é‡
    #if len(texts) > max_texts:
    #    print(f"âš ï¸ æ–‡æœ¬æ•¸é‡éå¤š ({len(texts)})ï¼Œé™åˆ¶ç‚º {max_texts} å€‹")
    #    texts = texts[:max_texts]
    
    # å°‡æ–‡æœ¬åˆ†æˆè¼ƒå°çš„æ‰¹æ¬¡
    batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
    all_embeddings = []
    valid_texts = []
    
    for i, batch in enumerate(batches):
        print(f"â–¶ï¸ è™•ç†æ‰¹æ¬¡ {i+1}/{len(batches)}ï¼Œå…± {len(batch)} å€‹æ–‡æœ¬")
        
        try:
            # å˜—è©¦æ‰¹é‡åµŒå…¥
            embeddings = embedding_model.embed_documents(batch)
            
            # æª¢æŸ¥åµŒå…¥çµæœæ˜¯å¦æœ‰æ•ˆ
            if len(embeddings) == len(batch):
                print(f"âœ… æ‰¹æ¬¡ {i+1} åµŒå…¥æˆåŠŸï¼Œç²å¾— {len(embeddings)} å€‹å‘é‡")
                all_embeddings.extend(embeddings)
                valid_texts.extend(batch)
            else:
                print(f"âš ï¸ æ‰¹æ¬¡ {i+1} åµŒå…¥çµæœæ•¸é‡ä¸ç¬¦ï¼Œé æœŸ {len(batch)}ï¼Œå¯¦éš› {len(embeddings)}")
                # åªä½¿ç”¨æˆåŠŸçš„éƒ¨åˆ†
                all_embeddings.extend(embeddings)
                valid_texts.extend(batch[:len(embeddings)])
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {i+1} åµŒå…¥è™•ç†å¤±æ•—: {str(e)}")
            traceback.print_exc()
        
        # æ‰¹æ¬¡ä¹‹é–“æ·»åŠ çŸ­æš«é–“éš”ï¼Œæ¸›è¼•è¨ˆç®—è² æ“”
        time.sleep(0.1)
    
    print(f"ğŸ“Š ç¸½è¨ˆ: è™•ç† {len(texts)} å€‹æ–‡æœ¬ï¼ŒæˆåŠŸåµŒå…¥ {len(all_embeddings)} å€‹")
    return valid_texts, all_embeddings

# âœ… Cold start / Init ç”¨ï¼šåˆå§‹åŒ–è³‡æ–™èˆ‡æ¨¡å‹
def do_init():
    global combined_data, embedding_model, vector_store, bm25

    JSON_FILE = "trade_tariff_database.json"
    PDF_FILES = ["td-codes.pdf", "finalCopy.pdf", "td-fields.pdf"]
    VECTOR_STORE_DIR = "vector_store"  # GCS ä¸­å­˜å„²å‘é‡åº«çš„ç›®éŒ„

    try:
        print("â–¶ï¸ å¾ GCS è¼‰å…¥ JSON èˆ‡ PDF...")
        database = load_json_from_gcs(GCS_BUCKET_NAME, JSON_FILE)
        pdf_data = load_pdfs_from_gcs(GCS_BUCKET_NAME, PDF_FILES)
        # combined_data = database + [{"content": text} for text in pdf_data]
        combined_data = [
            {**record, "content": record.get("brief_description", "")} for record in database
        ] + [{"content": text} for text in pdf_data]

    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–è³‡æ–™è¼‰å…¥å¤±æ•—ï¼š{e}")
        traceback.print_exc()
        return False

    if not combined_data:
        print("âŒ æœªè®€å–åˆ°ä»»ä½•æœ‰æ•ˆæ•¸æ“š")
        return False

    print("â–¶ï¸ åˆå§‹åŒ– HuggingFace æœ¬åœ°åµŒå…¥æ¨¡å‹...")
    try:
        # ä½¿ç”¨æœ¬åœ° HuggingFaceEmbeddings æ¨¡å‹
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"  
        )
        print("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        
        # æ¸¬è©¦åµŒå…¥æ¨¡å‹
        test_result = embedding_model.embed_query("æ¸¬è©¦åµŒå…¥æ¨¡å‹")
        if isinstance(test_result, list) and len(test_result) > 0:
            print(f"âœ… åµŒå…¥æ¨¡å‹æ¸¬è©¦æˆåŠŸï¼Œå‘é‡ç¶­åº¦: {len(test_result)}")
        else:
            print(f"âš ï¸ åµŒå…¥æ¨¡å‹æ¸¬è©¦çµæœæ ¼å¼ç•°å¸¸: {type(test_result)}")
            embedding_model = None
            
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        traceback.print_exc()
        embedding_model = None
    
    # æª¢æŸ¥æ˜¯å¦å­˜åœ¨é å…ˆå»ºç«‹çš„å‘é‡åº«
    if embedding_model:
        print("â–¶ï¸ æª¢æŸ¥ GCS ä¸­æ˜¯å¦å­˜åœ¨å‘é‡åº«æ–‡ä»¶...")
        if vector_store_exists_in_gcs(GCS_BUCKET_NAME, VECTOR_STORE_DIR):
            print("âœ… ç™¼ç¾å‘é‡åº«æ–‡ä»¶ï¼Œæ­£åœ¨å˜—è©¦åŠ è¼‰...")
            
            # è¨˜éŒ„ç•¶å‰æ™‚é–“ï¼Œç”¨æ–¼è¨ˆç®—åŠ è¼‰æ™‚é–“
            start_time = time.time()
            
            vector_store = load_vector_store_from_gcs(embedding_model, GCS_BUCKET_NAME, VECTOR_STORE_DIR)
            print(f"ğŸ§ª vector_store å‹åˆ¥: {type(vector_store)}ï¼Œæ˜¯å¦ç‚º None: {vector_store is None}")

            
            if vector_store:
                load_time = time.time() - start_time
                print(f"âœ… æˆåŠŸåŠ è¼‰å‘é‡åº«ï¼Œè€—æ™‚ {load_time:.2f} ç§’")
                
                # å˜—è©¦æª¢æŸ¥å‘é‡ç¶­åº¦æ˜¯å¦åŒ¹é…
                test_embedding = embedding_model.embed_query("æ¸¬è©¦")
                expected_dim = len(test_embedding)
                
                # å˜—è©¦å¾å‘é‡åº«ç²å–ä¸€å€‹å‘é‡ä¾†æª¢æŸ¥ç¶­åº¦
                try:
                    # ä½¿ç”¨ä¸€å€‹ç°¡å–®æŸ¥è©¢ä¾†æª¢æŸ¥å‘é‡
                    test_results = vector_store.similarity_search("test", k=1)
                    if test_results:
                        print(f"âœ… å‘é‡åº«å¯ä»¥æ­£å¸¸æœç´¢ï¼Œè¼‰å…¥å®Œæˆ")
                    else:
                        print("âš ï¸ å‘é‡åº«åŠ è¼‰æˆåŠŸï¼Œä½†æœç´¢æ¸¬è©¦è¿”å›ç©ºçµæœ")
                except Exception as e:
                    print(f"âš ï¸ å‘é‡åº«åŠ è¼‰å¾Œï¼Œæœç´¢æ¸¬è©¦å¤±æ•—: {str(e)}")
                    print("âš ï¸ ç¹¼çºŒä½¿ç”¨å‘é‡åº«ï¼Œä½†å¯èƒ½å­˜åœ¨å…¼å®¹æ€§å•é¡Œ")
            else:
                print("âš ï¸ åŠ è¼‰å‘é‡åº«å¤±æ•—ï¼Œå°‡é‡æ–°å»ºç«‹")
                vector_store = None
        else:
            print("â„¹ï¸ æœªæ‰¾åˆ°ç¾æœ‰å‘é‡åº«ï¼Œå°‡å»ºç«‹æ–°çš„å‘é‡åº«")
            vector_store = None
        if not vector_store:
            print("âš ï¸ vector_store is None after load, preparing to rebuild...")
    
    # å¦‚æœæ²’æœ‰åŠ è¼‰æˆåŠŸï¼Œå‰‡é‡æ–°å»ºç«‹å‘é‡åº«
    if embedding_model and not vector_store:
        # æº–å‚™æ–‡æœ¬æ•¸æ“š
        text_data = [record.get("content", "") for record in combined_data]
        text_splits = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_text("\n".join(text_data))
        print(f"â–¶ï¸ æ–‡æœ¬æ‹†åˆ†å®Œæˆï¼Œå…± {len(text_splits)} å€‹ç‰‡æ®µ")
        
        # é™åˆ¶è™•ç†çš„æ–‡æœ¬æ•¸é‡ï¼Œé¿å…è¶…æ™‚
        #max_texts = 8000  # å¯ä»¥æ ¹æ“šæ‚¨çš„ Cloud Function è³‡æºä¾†èª¿æ•´
        #if len(text_splits) > max_texts:
        #    print(f"âš ï¸ é™åˆ¶è™•ç†çš„æ–‡æœ¬æ•¸é‡ç‚º {max_texts}ï¼ˆåŸæœ‰ {len(text_splits)} å€‹ï¼‰")
        #    text_splits = text_splits[:max_texts]
        
        try:
            # ä½¿ç”¨å®‰å…¨çš„æ–¹å¼è™•ç†åµŒå…¥
            valid_texts, embeddings = safe_embed_documents(embedding_model, text_splits, batch_size=8)
            
            if valid_texts and embeddings:
                # å»ºç«‹å‘é‡åº«
                text_embeddings = list(zip(valid_texts, embeddings))
                try:
                    vector_store = FAISS.from_embeddings(text_embeddings, embedding_model)
                    print(f"âœ… æˆåŠŸå»ºç«‹å‘é‡åº«ï¼ŒåŒ…å« {len(text_embeddings)} å€‹åµŒå…¥")
                    
                    # å°‡å‘é‡åº«ä¿å­˜åˆ° GCS
                    print("â–¶ï¸ å°‡å‘é‡åº«ä¿å­˜åˆ° GCS...")
                    if save_vector_store_to_gcs(vector_store, GCS_BUCKET_NAME, VECTOR_STORE_DIR):
                        print("âœ… å‘é‡åº«å·²æˆåŠŸä¿å­˜åˆ° GCSï¼Œä¸‹æ¬¡å°‡ç›´æ¥åŠ è¼‰")
                    else:
                        print("âš ï¸ å‘é‡åº«ä¿å­˜å¤±æ•—ï¼Œä½†æœ¬æ¬¡ä»å¯ä½¿ç”¨")
                        
                except Exception as e:
                    print(f"âŒ å»ºç«‹å‘é‡åº«å¤±æ•—: {str(e)}")
                    traceback.print_exc()
                    vector_store = None
            else:
                print("âŒ æ²’æœ‰æœ‰æ•ˆçš„åµŒå…¥çµæœï¼Œç„¡æ³•å»ºç«‹å‘é‡åº«")
                vector_store = None
        except Exception as e:
            print(f"âŒ åµŒå…¥è™•ç†å¤±æ•—: {str(e)}")
            traceback.print_exc()
            vector_store = None
    else:
        if not embedding_model:
            print("âš ï¸ åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³éå‘é‡åº«å»ºç«‹/åŠ è¼‰")
    
    # å»ºç«‹ BM25 ç´¢å¼•ï¼ˆä½œç‚ºå‚™ç”¨æœç´¢æ–¹æ³•ï¼‰
    print("â–¶ï¸ å»ºç«‹ BM25 é—œéµå­—æœå°‹ç´¢å¼•...")
    try:
        # bm25_corpus = [record.get("content", "") for record in combined_data if "content" in record]
        bm25_corpus = [record.get("brief_description", "") for record in combined_data if "brief_description" in record]
        if bm25_corpus:
            tokenized_corpus = [doc.split() for doc in bm25_corpus if doc.strip()]
            if tokenized_corpus:
                bm25 = BM25Okapi(tokenized_corpus)
                print(f"âœ… æˆåŠŸå»ºç«‹ BM25 ç´¢å¼•ï¼ŒåŒ…å« {len(tokenized_corpus)} å€‹æ–‡æª”")
            else:
                print("âŒ æ²’æœ‰æœ‰æ•ˆçš„åˆ†è©æ–‡æœ¬ï¼Œç„¡æ³•å»ºç«‹ BM25 ç´¢å¼•")
                bm25 = None
        else:
            print("âŒ æ²’æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å…§å®¹ï¼Œç„¡æ³•å»ºç«‹ BM25 ç´¢å¼•")
            bm25 = None
    except Exception as e:
        print(f"âŒ å»ºç«‹ BM25 ç´¢å¼•å¤±æ•—: {str(e)}")
        traceback.print_exc()
        bm25 = None
    
    # å¦‚æœæ‰€æœ‰ç´¢å¼•éƒ½å¤±æ•—ï¼Œå‰‡è¿”å›å¤±æ•—
    if not vector_store and not bm25:
        print("âŒ å‘é‡åº«å’Œ BM25 ç´¢å¼•éƒ½åˆå§‹åŒ–å¤±æ•—ï¼Œç³»çµ±å°‡ç„¡æ³•æ­£å¸¸å·¥ä½œ")
        return False
    
    print("âœ… åˆå§‹åŒ–å®Œæˆ")
    return True

# âœ… Webhookï¼šLINE è¨Šæ¯å…¥å£
@app.route("/callback", methods=['POST'])
def callback(request):
    signature = request.headers.get('X-Line-Signature', '')
    body = request.get_data(as_text=True)
    print(f"âœ… æ”¶åˆ° LINE Webhook è«‹æ±‚")
    
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        print(f"âŒ LINE ç°½åé©—è­‰å¤±æ•—")
        abort(400)
    except Exception as e:
        print(f"âŒ LINE Webhook è™•ç†å¤±æ•—: {str(e)}")
        traceback.print_exc()
        abort(500)
    
    return 'OK'

# âœ… æ‰‹å‹•åˆå§‹åŒ– API
@app.route("/init", methods=['POST'])
def initialize():
    success = do_init()
    if success:
        return jsonify({"status": "success", "msg": "è³‡æ–™èˆ‡æ¨¡å‹åˆå§‹åŒ–å®Œæˆ"})
    else:
        return jsonify({"status": "fail", "msg": "åˆå§‹åŒ–å¤±æ•—"}), 500

# âœ… å¼·åˆ¶é‡å»ºå‘é‡åº« API
@app.route("/rebuild-vectors", methods=['POST'])
def rebuild_vectors():
    """å¼·åˆ¶é‡å»ºå‘é‡åº«çš„ API ç«¯é»"""
    global vector_store, embedding_model
    
    # æª¢æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å·²åˆå§‹åŒ–
    if not embedding_model:
        return jsonify({
            "status": "error", 
            "msg": "åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå‘¼å« /init"
        }), 400
    
    try:
        # æº–å‚™æ–‡æœ¬æ•¸æ“š
        text_data = [record.get("content", "") for record in combined_data]
        text_splits = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_text("\n".join(text_data))
        
        # é™åˆ¶è™•ç†çš„æ–‡æœ¬æ•¸é‡ï¼Œé¿å…è¶…æ™‚
        #max_texts = 8000  # å¯ä»¥æ ¹æ“šæ‚¨çš„ Cloud Function è³‡æºä¾†èª¿æ•´
        #if len(text_splits) > max_texts:
        #    text_splits = text_splits[:max_texts]
        
        # ä½¿ç”¨å®‰å…¨çš„æ–¹å¼è™•ç†åµŒå…¥
        valid_texts, embeddings = safe_embed_documents(embedding_model, text_splits, batch_size=8)
        
        if not valid_texts or not embeddings:
            return jsonify({
                "status": "error", 
                "msg": "åµŒå…¥è™•ç†å¤±æ•—ï¼Œæ²’æœ‰ç²å–åˆ°æœ‰æ•ˆçš„åµŒå…¥å‘é‡"
            }), 500
        
        # å»ºç«‹å‘é‡åº«
        text_embeddings = list(zip(valid_texts, embeddings))
        vector_store = FAISS.from_embeddings(text_embeddings, embedding_model)
        
        # å°‡å‘é‡åº«ä¿å­˜åˆ° GCS
        VECTOR_STORE_DIR = "vector_store"
        save_success = save_vector_store_to_gcs(vector_store, GCS_BUCKET_NAME, VECTOR_STORE_DIR)
        
        if not save_success:
            return jsonify({
                "status": "partial_success", 
                "msg": "å‘é‡åº«å·²é‡å»ºä½†ä¿å­˜å¤±æ•—ï¼Œé‡å•Ÿå¾Œå°‡éœ€è¦é‡æ–°å»ºç«‹"
            })
        
        return jsonify({
            "status": "success", 
            "msg": f"æˆåŠŸé‡å»ºä¸¦ä¿å­˜å‘é‡åº«ï¼ŒåŒ…å« {len(text_embeddings)} å€‹åµŒå…¥å‘é‡"
        })
        
    except Exception as e:
        traceback.print_exc()
        return jsonify({
            "status": "error", 
            "msg": f"é‡å»ºå‘é‡åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        }), 500

# âœ… å¥åº·æª¢æŸ¥ API
@app.route("/healthz", methods=['GET'])
def healthz():
    return "OK"

@app.route("/", methods=['GET'])
def index():
    return "Service is running"

# âœ… æ¸…ç†æŸ¥è©¢å­—ä¸²ï¼ˆç§»é™¤éå­—è©é¡çš„å¹²æ“¾ï¼‰
def clean_query(text):
    return re.sub(r'[^\w\s]', '', text.lower())
    
# âœ… è³‡æ–™æª¢ç´¢é‚è¼¯
def retrieve_relevant_records(query):
    query_cleaned = clean_query(query)
    exact_matches = [record for record in combined_data if record.get("hts8") == query.strip()]

    keyword_matches = []
    if bm25:
        try:
            print(f"â–¶ï¸ åŸ·è¡Œ BM25 é—œéµå­—æœå°‹...")
            tokenized_query = query_cleaned.split()
            bm25_corpus = [record.get("brief_description", "") for record in combined_data if "brief_description" in record]
            tokenized_corpus = [doc.split() for doc in bm25_corpus]
            bm25_index = BM25Okapi(tokenized_corpus)
            bm25_scores = bm25_index.get_scores(tokenized_query)
            top_indexes = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:5]
            keyword_matches = [combined_data[i] for i in top_indexes if i < len(combined_data)]
            print(f"âœ… BM25 æœå°‹æ‰¾åˆ° {len(keyword_matches)} å€‹åŒ¹é…é …")
        except Exception as e:
            print(f"âš ï¸ BM25 æœå°‹å¤±æ•—: {str(e)}")
            traceback.print_exc()

    vector_matches = []
    if vector_store:
        try:
            print(f"â–¶ï¸ åŸ·è¡Œå‘é‡ç›¸ä¼¼åº¦æœå°‹...")
            faiss_results = vector_store.similarity_search(query, k=5)
            vector_matches = [{"content": doc.page_content} for doc in faiss_results]
            print(f"âœ… å‘é‡æœå°‹æ‰¾åˆ° {len(vector_matches)} å€‹åŒ¹é…é …")
        except Exception as e:
            print(f"âš ï¸ å‘é‡æœå°‹å¤±æ•—: {str(e)}")
            traceback.print_exc()
    else:
        print("âš ï¸ vector_store ç‚º Noneï¼Œç„¡æ³•åŸ·è¡Œèªæ„æœå°‹")

    # âœ… Fuzzy matching å¢å¼·æª¢ç´¢
    fuzzy_matches = []
    try:
        print("â–¶ï¸ åŸ·è¡Œ FuzzyWuzzy æ¨¡ç³Šå­—ä¸²åŒ¹é…...")
        threshold = 80
        for record in combined_data:
            desc = record.get("brief_description", "")
            score = fuzz.partial_ratio(query_cleaned, desc.lower())
            if score >= threshold:
                fuzzy_matches.append(record)
        print(f"âœ… FuzzyWuzzy æ‰¾åˆ° {len(fuzzy_matches)} å€‹æ¨¡ç³ŠåŒ¹é…é …")
    except Exception as e:
        print(f"âš ï¸ FuzzyWuzzy åŸ·è¡Œå¤±æ•—: {str(e)}")
        traceback.print_exc()

    results = list({json.dumps(r, ensure_ascii=False, default=json_serializable) for r in (exact_matches + keyword_matches + vector_matches + fuzzy_matches)})
    return [json.loads(r) for r in results] if results else [{"message": "æœªæ‰¾åˆ°ç›¸é—œè³‡è¨Š"}]

# âœ… ä½¿ç”¨ Mistral API ç”Ÿæˆå›æ‡‰
def generate_reply(message):
    # æª¢æŸ¥ç³»çµ±æ˜¯å¦å·²åˆå§‹åŒ–
    if not (embedding_model or bm25):
        return "âš ï¸ ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹å…ˆå‘¼å« /init"

    try:
        translated_query = translate_to_english(message)
        # æª¢ç´¢ç›¸é—œè¨˜éŒ„
        retrieved_data = retrieve_relevant_records(message)
        
        # æº–å‚™ Mistral API è«‹æ±‚
        client = Mistral(api_key=MISTRAL_API_KEY)
        
        # å°‡æª¢ç´¢çµæœè½‰æ›ç‚ºä¸Šä¸‹æ–‡
        context = json.dumps(retrieved_data, ensure_ascii=False, indent=2)
        
        # æ§‹å»ºæç¤ºè©
        prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¾åœ‹åœ‹éš›è²¿æ˜“å§”å“¡æœƒï¼ˆUnited States International Trade Commission / USITCï¼‰é—œç¨…æ•¸æ“šå°ˆå®¶åŠ©æ‰‹ã€‚ä½ çš„ä»»å‹™æ˜¯å¾æä¾›çš„åƒè€ƒè³‡æ–™ä¸­ï¼Œæº–ç¢ºä¸”æ¸…æ™°åœ°å›ç­”èˆ‡é—œç¨…ã€è²¿æ˜“ç›¸é—œçš„å•é¡Œã€‚
                    å›ç­”å•é¡Œæ™‚ï¼Œè«‹éµå¾ªä»¥ä¸‹å›æ‡‰æŒ‡å°åŸå‰‡ï¼š
                    1. è³‡æ–™ä¾†æºèˆ‡å¯é æ€§ï¼š
                        -åƒ…ä½¿ç”¨æä¾›çš„åƒè€ƒè³‡æ–™å…§å®¹ä½œç‚ºå›ç­”ä¾æ“šï¼Œå¦‚æœè³‡æ–™ä¸è¶³ï¼Œæ˜ç¢ºèªªæ˜ç„¡æ³•å®Œå…¨å›ç­”
                        -å„ªå…ˆæä¾›èˆ‡ç”¨æˆ¶å•é¡Œæœ€é—œéµå’Œæœ€ç›¸é—œçš„è³‡è¨Š
                    2. å›ç­”æ ¼å¼è¦æ±‚ï¼š
                        - è«‹ä½¿ç”¨åˆ—é»æ ¼å¼
                        - é¿å…ä½¿ç”¨ç²—é«”æˆ–æ–œé«”
                        - å›ç­”å­—æ•¸å¤§ç´„120å­—å·¦å³
                        - å›ç­”ä¸­æ–‡çš„è©±è«‹ä½¿ç”¨å°ç£ç”¨èª

                    3. å›ç­”å¿…é ˆåŒ…å«
                        - ç”¨æˆ¶å•é¡ŒæåŠé …ç›®çš„é—œç¨…æ•¸æ“š
                        - åœ‹å®¶ä»£ç¢¼çš„å°æ‡‰åœ‹å®¶åç¨±
                        - ç›¸é—œçš„ MFNï¼ˆæœ€æƒ åœ‹ï¼‰é—œç¨…ä¿¡æ¯
                        - å¯èƒ½çš„ç”Ÿæ•ˆæ—¥æœŸ

                    4. é™åˆ¶æ¢ä»¶
                    - åš´æ ¼é™å®šåœ¨é—œç¨…å’Œåœ‹éš›è²¿æ˜“é ˜åŸŸ
                    - å°æ–¼è¶…å‡ºç¯„åœçš„å•é¡Œï¼Œç¦®è²Œåœ°æ‹’çµ•å›ç­”

                    ç”¨æˆ¶å•é¡Œ: {message}

                    åƒè€ƒè³‡æ–™:
                    {context}

                    è«‹æ ¹æ“šä¸Šè¿°æŒ‡å°åŸå‰‡ï¼Œæä¾›ä¸€å€‹å°ˆæ¥­ã€ç²¾ç¢ºä¸”æ˜“æ–¼ç†è§£çš„å›ç­”ï¼Œå¤§ç´„120å­—ã€‚"""

        # ç™¼é€è«‹æ±‚åˆ° Mistral API
        messages = [{"role": "user", "content": prompt}]
        
        try:
            chat_response = client.chat.complete(
                model="mistral-small-latest", 
                messages=messages, 
                temperature=0.5,
                max_tokens=300
            )
            return chat_response.choices[0].message.content
        except Exception as e:
            print(f"âŒ Mistral API èª¿ç”¨å¤±æ•—: {str(e)}")
            traceback.print_exc()
            return f"âš ï¸ ç„¡æ³•ç”Ÿæˆå›æ‡‰ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚æŠ€è¡“éŒ¯èª¤: {str(e)[:100]}..."
    
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        traceback.print_exc()
        return "âš ï¸ è™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

# âœ… ç¨‹å¼é€²å…¥é»ï¼šåˆå§‹åŒ–ç³»çµ±
check_env()
setup_linebot()
print("ğŸ“¦ æ­£åœ¨è‡ªå‹•åˆå§‹åŒ–ç³»çµ±è³‡æ–™...")
do_init()
register_handler()

# âœ… æœ¬åœ°æ¸¬è©¦ç”¨
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8080))
    app.run(host="0.0.0.0", port=port)